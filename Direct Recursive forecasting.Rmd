---
title: "Assignment 6"
author: "Aamer hussain"
date: "2/19/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
setwd("/Users/mohammedhussain/Desktop/UCHICAGO assigments/Time series/Assignment 6")
```


```{r}
install.packages('ggplot2', repos = "http://cran.us.r-project.org")
```

```{r}
library("forecast")
library("xts")
library("TSA")
library(dplyr)
library(reshape2)
library(ggplot2)
library(tidyverse)
library(ggthemes)
library(tsibble)
library(zoo)
library(tseries)
```

```{r}
library("TSA")
data(beersales)
```
The data is the monthly beer sales in millions of barrels, 01/1975 - 12/1990.

```{r}
beersales
```


 # lets check if the time series is stationary 
```{r}
adf.test(beersales)
```

 Since the p-value = 0.01, we can reject Ho:non-stationary TS
 
 This beer sales time series is a stationary time series .

```{r}
kpss.test(beersales, null= "Trend")
```

It fails to reject the null hypothesis which is stated as the Time series is trend stationary . We can use ARIMA to handle such a trend stationary TS.

Since the p-value = 0.1, we do not reject Ho:stationary TS.

Based on the results from the ADF test and KPSS test , we can conclude that the beer sales time series is stationary 



## Seasonal decomposition
```{r}
plot(decompose(beersales))
```



```{r}
#Train: 01/1975 - 12/1989.
train.data <- window(beersales, end = c(1989,12))
#Test: 1990
test.data <- window(beersales, start = c(1990,1))
```


```{r}
train.data
```

```{r}
test.data
```

### Part 1 - use ARIMA(p,d,q) model to forecast beer sales for all months of 1990 using the following two multi-step forecasting approaches. For each model, check mean, autocorrelation and normality of the residuals. Confirm if the residuals are white noise.

## 1A - Use the h-period in forecast() to forecast each month of 1990. This is also known as recursive forecasting where you fit a model only once and use it recursively for h-periods.


```{r}
begin.time <- Sys.time()
recursive.model <- auto.arima(train.data, stepwise=FALSE, approximation = FALSE)
forecast.recursive <- forecast(recursive.model, h=12)
end.time <- Sys.time()
rec.comp.time <- end.time - begin.time
rec.comp.time
```

# forecasting
```{r}

```

```{r}
summary(recursive.model)
```

```{r}
mean(recursive.model$residuals)
```

We clearly see that The mean of the residuals is not equal to 0 which suggests that this recursive model is a biased model. The recursive model has been overestimating on a mean/average of 0.04335538 to all of its forecasts.

```{r}
ts.plot(recursive.model$residuals)
```

This plot shows that the residuals of this recursive model is not perfectly close to being white noise.

```{r}
acf(recursive.model$residuals)
```

There is minimum level  of correlation or no correlation among the residuals of the recursive model.


```{r}
pacf(recursive.model$residuals)
```


```{r}
tsdisplay(recursive.model$residuals)
```


There is some level of autocorrelation at higher lags but not at the lower lags .

## Ljung box test for the autocorrelation among the residuals of the recurisve model
```{r}
Box.test(resid(recursive.model), type = "Ljung-Box")
```

The p value as an outcome of the Ljung box test for residuals is 0.5 which is more than 0.05 . This means that the residuals are not autocorrelated.

```{r}
hist(recursive.model$residuals)
```

The residuals have a skewed distribution which is not perfectly normal. The residuals are not normal. we will perform shapiro wilk normality test later .

The residuals from this recursive model are close to being white noise or almost white noise. The residuals are similar to white noise.


## Shapiro wilk test for Normality for the recursive model
```{r}
shapiro.test(recursive.model$residuals)
```


The p value from the shapiro wilk test suggest that the residuals of the recursive model are not perfectly normal 

```{r}
forecast.recursive
```

## Plotting the forecast
```{r}
plot(forecast(forecast.recursive, 12),xlab = "Month", ylab = "Beer Sales")
lines(x = c(181:192), y = test.data, col = "red")
```


## 1B - Use the monthly data as a continuous time series. Forecast for 1990 Jan, Plug forecast into the time series, build a new model to forecast for 1990 Feb. And so on and so forth. In other words, h=1 in all the forecasts. This is known as direct recursive (DirRec) forecasting where you fit a new model for each time step.


```{r}
direc.train.data <- train.data
direc.forecast.list <- c()
direc.models.list <- list()
begin.time <-Sys.time() #start timing
for(i in 1:12) {
  direc.model <- auto.arima(direc.train.data)
  direc.models.list[[i]] <- direc.model
  direc.forecast <- forecast(direc.model, h=1)$mean
  direc.forecast.list  <- c(direc.forecast.list,  direc.forecast)
  direc.train.data   <- ts(c(direc.train.data, direc.forecast), start=1975, frequency=12)
}
end.time <- Sys.time() 
dir.rec.compute.time <-  end.time - begin.time 
dir.rec.compute.time
```


```{r}
forecast.compare <- data.frame(
  month <- c(1:12), 
  actuals <- test.data, 
  recursive <- forecast.recursive$mean,
  direct.recursive <- direc.forecast.list
  ) %>% mutate(
  di.rec.residuals <- direct.recursive - actuals,
  recursive.residuals <- recursive - actuals
  )

```


```{r}
names(forecast.compare) <- c("Months","Actual","Recursive","Direct recursive","residuals.Recursive","residuals.Direct.Recursive")
head(forecast.compare, 12)
```


## 1C - Plot the mean, the p-value of the autocorrelation test and the p-value of the normality test of the residuals of the 12 models from the Direct recursive approach

```{r}
di.rec.resid.df = plyr::ldply(direc.models.list, function(i) {
  data.frame(
    residuals.mean = mean(i$residuals), 
    autocorrelation.pval = Box.test(i$residuals, type='Ljung-Box')$p.value,
    normality.pval = shapiro.test(i$residuals)$p.value
  )
})
plot(di.rec.resid.df$residuals.mean, main="Mean of Residuals", xlab="Model Number", type = "l", ylab = "Residual mean")

```


The mean of the residuals are gradually approaching 0 as the model number from the direct recursive approach increases. This indicates that the higher model closer to 12 are able to model the beer sales data better leaving the residuals to be closer to the the white noise.

## Ljung box test for the autocorrelation test of the residuals


```{r}
plot(di.rec.resid.df$autocorrelation.pval, main="P-Value of Ljung-Box Autocorrelation Test of Residuals", xlab="Model Number", type = "l", ylab = "Ljung-Box Autocorrelation p value")

```

The p values  from the Ljung box  autocorrelation test for the direct recursive approach gradually decreases with model number which means the level of autocorrelation among the residuals is going down . The residuals does seem to be correlated .


## Plotting the p values from the Shapiro wilk test for normality on all the model of direct recursive approach

```{r}
plot(di.rec.resid.df$normality.pval, main="P-Value of Shapiro Wilk Normality Test of Residuals", xlab="Model Number", type = "l", ylab = "Shapiro Wilk Normality Test p value")
```


This plot shows that the structure of the residuals as the model number increases looks more and more normal.
This means that the residuals are becoming more and more normal as the model number increases with the direct recursive approach .

# We will do more visualizations

```{r}
x <- seq(1,12)
plot(x,di.rec.resid.df$residuals.mean,
     type='o',
     ylim = c(-.05,0),
     xlab = "Model Number",
     ylab = " Residuals of the Model",
     main = "Mean of Residuals")
```


```{r}
x <- seq(1,12)
plot(x, di.rec.resid.df$autocorrelation.pval,
     type='o',
     ylim = c(0.978,0.984),
     xlab = "Model Number",
     ylab = " Ljung-Box Test P-value",
     main = "P-value of Ljung-Box Test")
legend("top", inset = .01, box.lty=0, legend = ("Ho: Residuals are independent and not autocorrelated"))
```


```{r}
x <- seq(1,12)
plot(x, di.rec.resid.df$normality.pval,
     type='o',
     xlab = "Model Number",
     ylab = "P-value",
     main = " Shapiro Wilk Test - Pvalue")
legend("top", inset = .01, box.lty=0, legend = ("Ho: Residuals have same distribution as the Normal Distribution"))
```

## ACF plots for all of the 12 models in the direct recurisve approach
```{r}
par(mfrow=c(3,4))
x<-1
for (i in 1:12){
  acf(di.rec.resid.df$residuals.mean,
       main=paste("Model",x,"residuals"),
       xlab = "")
  x <- x + 1
}

```

There is not much correlation among the residuals for all of the 12 models in the direct recurisve approach . Residuals look like white noise.

## Histograms
```{r}
par(mfrow=c(3,4))
x<-1
for (i in 1:12){
  hist(di.rec.resid.df$residuals.mean,
       main=paste("Model",x,"residuals"),
       xlab = "")
  x <- x + 1
}

```

```{r}
for (i in 1:12) {
  sum.residuals <- sum(mean(direc.models.list[[i]]$residuals))
}
sum.residuals
```


There is a negative skew to the residuals. The residuals are almost but not perfectly normal. Based on the shapiro wilk test , we can say that the residuals are normal.

The residuals of all the 12 models of the direct recursive approach do not have a mean of 0. But its closer to zero They do not have auto correlation . 


```{r}
par(mfrow=c(3,4))
x<-1
for (i in 1:12){
  qqnorm(di.rec.resid.df$residuals.mean,
         main=paste("Model",x,"residuals"),
         ylim = c(-.02,-0.01),
         xlab = "",
         asp=1)
  qqline(di.rec.resid.df$residuals.mean, col = "red")
  x <- x + 1
}
```





```{r}
forecast.direct.recursive <- ts(forecast.compare$`Direct recursive`)
```


### 2 - Plot the Recursive and DirRec along with the actuals. Use ylim=c(12.5, 17) to get a good visual of the plot differences.


```{r}
forecasted_values <- test.data %>% as_tsibble() %>% mutate(direct_recursive = forecast.direct.recursive,
         recursive_model = forecast.recursive %>% as_tibble() %>% pull(`Point Forecast`))
forecasted_values %>% rename(actual = value) %>% 
  gather(key = type, value = value) %>% 
  ggplot(aes(index, value, color = type)) +
  geom_point() +
  geom_line() +
  scale_color_viridis_d(option  ="plasma",name = NULL, labels = c("Actual", "Direct Recursive", "Recursive")) +
  labs(title = "Actual vs Direct recursive vs Recursive",
       x = "Date",
       y = "beer sales in millions of barrels")
```


### 3 - Calculate the MSE for 1990 - which of the two approaches take larger computation time and why?

```{r}
MSE.recursive <- mean((forecast.recursive$mean - test.data)^2)
MSE.direct.recursive <- mean((forecast.compare$`Direct recursive` - test.data)^2)
MSE.compare <- data.frame(cbind(MSE.recursive,MSE.direct.recursive))
MSE.compare
```




```{r}
rec.comp.time
```


```{r}
dir.rec.compute.time
```


```{r}
compare.computation <-  data.frame(rbind(as.numeric(rec.comp.time),as.numeric(dir.rec.compute.time)))
rownames(compare.computation) <- c("Recursive ","Direct recursive")
names(compare.computation) <- c("Computation time")
compare.computation
```


The direct recursive method or model clearly stands out as the one that took immense amount of more time, 46.16002 seconds compared to 22.01841 seconds of the recursive approach ( which also included the forecasting step ). 
Direct recursive approach also yielded a larger Mean squared error value (MSE). This means the recurive model performed better than the direct recursive model.

Direct recurisve model was more expensive operation/process in terms of the computation because it requires the building of many individual models as periods we want in order to do the forecasting .

I would prefer the recursive model instead of direct recursive because increase in the computation tike is not worth it because it does not lead to increase in accuracy or decrease in errors(MSE).


The direct recursive model approach  takes a significantly much more time compared to the recursive model approach because it creates multiple models where we fit a new model for each time step versus the  fitting a single model only once and use it for forecasting recursively for 12-periods/months.


### Part B


```{r}
df_cmeS <- read.csv("cmeS.csv")
df_immS <- read.csv("immS.csv")
df_iomS <- read.csv("iomS.csv")
```



```{r}
head(df_cmeS,20)
```


```{r}
head(df_immS)
```


```{r}
head(df_iomS,20)
```



```{r}
df_cmeS <- read.csv("cmeS.csv")
df_immS <- read.csv("immS.csv")
df_iomS <- read.csv("iomS.csv")
```

Before counting the missing values for each of the classes , we have to group them by month 
```{r}
cme <- df_cmeS 
imm <- df_immS  
iom <- df_iomS 
```



```{r}
function_month_groupby <- function(ts) {
  # Formatting of the date
  ts$DateOfSale <- as.Date(ts$DateOfSale, '%m/%d/%Y')
  ts$yearmon <- as.yearmon(ts$DateOfSale)
  
  # Grouping by month
  ts <- ts %>%
    group_by(yearmon) %>%
    summarize(price=mean(price)) %>%
    data.frame() %>%
    merge(data.frame(yearmon = as.yearmon(2001 + seq(0, 12*13-1)/12)),all=T)

}
  
cme <- function_month_groupby(df_cmeS)
imm <- function_month_groupby(df_immS)
iom <- function_month_groupby(df_iomS)
```

After grouping these seat prices data by month , we get 156 rows for each seat prices data

## Counting the missing rows/values for each for each of the seat prices 


```{r}
cme
```

# missing values for cmeS
```{r}
sum(is.na(cme))
```

There are 22 missing values for months in the cmeS data

```{r}
imm
```



```{r}
sum(is.na(imm))
```

There are 10 missing values for months in the immS data

```{r}
iom
```


```{r}
sum(is.na(iom))
```

There are 9 missing values for months in the iomS data

The process of filling the missing values within the data is called imputation.




We can perform interpolation via 3 important approaches that are commonly used. I have used this link as reference :
https://www.xspdf.com/resolution/53630765.html

These 3 approaches are : linear , constant and spline

1. constant interpolation : This approach takes the previous value and populates the missing value with the previous values

2. linear interpolation : This approach takes the preceding value to the missing value and the succeeding value to the missing value and draws a line connecting the preceding value and the succeeding value. basically this approach takes the average.

3. Spline interpolation : This approach takes into consideration the cublic spline function 


# Lets create a funcion that creates a uniform time series by formatting the date , grouping by month and then perform interpllation for the missing values using the 3 approaches : linear , constant  and spline
```{r}
create_uniform_ts <- function(k, interpolation='spline') {
  # Formatting of the date
  k$DateOfSale <- as.Date(k$DateOfSale, '%m/%d/%Y')
  k$yearmon <- as.yearmon(k$DateOfSale)
  
  # Grouping by month
  k <- k %>%
    group_by(yearmon) %>%
    summarize(price=mean(price)) %>%
    data.frame() %>%
    merge(data.frame(yearmon = as.yearmon(2001 + seq(0, 12*13-1)/12)),all=T)
  
  # perform Interpolation
  k$interpolated <- is.na(k$price)
  x = k$yearmon
  if('constant' %in% interpolation) {
    k$constant <- approx(k$yearmon, k$price, xout=x, method = "constant")$y  
  }
  if('linear' %in% interpolation) {
    k$linear <- approx(k$yearmon, k$price, xout=x)$y
  }
  if('spline' %in% interpolation) {
    k$spline <- spline(k, n=nrow(k))$y
  }
  k
}

```




## Creating the uniform time series using the linear interpolation 
```{r}
uniform_cmeS_linear <- create_uniform_ts(df_cmeS,interpolation = c('linear'))
uniform_immS_linear <- create_uniform_ts(df_immS, interpolation = c('linear'))
uniform_iomS_linear <- create_uniform_ts(df_iomS, interpolation = c('linear'))

```

# lets see if there are any missing values after the linear interpolation for the CME seat prices data
```{r}
sum(is.na(uniform_cmeS_linear$linear))
```

# lets see if there are any missing values after the linear interpolation for the IMM seat prices data
```{r}
sum(is.na(uniform_immS_linear$linear))
```

# lets see if there are any missing values after the linear interpolation for the IOM seat prices data
```{r}
sum(is.na(uniform_iomS_linear$linear))
```


### Plotting the uniform time series where the missing values were filled in using the 'linear' interpolation 

# plotting the data for the CME seat prices after linear interpolation
```{r}
ggplot(uniform_cmeS_linear, aes(x=yearmon, y=linear, color=interpolated)) + 
  geom_point()  + scale_color_manual(values=c('red', 'green')) + 
  ggtitle('CME Seat Price: 2001-2013')

```

If we clearly observe the linear approach fairly performs a good job of interpolation/ filling in missing values  for CME seat prices 

# plotting the data for the IMM seat prices after linear interpolation
```{r}
ggplot(uniform_immS_linear, aes(x=yearmon, y=linear, color=interpolated)) + 
  geom_point() + scale_color_manual(values=c('red', 'green')) +
  ggtitle('IMM Seat Price: 2001-2013')

```


If we clearly observe the linear approach fairly performs a good job of interpolation/ filling in missing values for IMM seat prices 


# plotting the data for the IOM seat prices after linear interpolation
```{r}
ggplot(uniform_iomS_linear, aes(x=yearmon, y=linear, color=interpolated)) + 
  geom_point() + scale_color_manual(values=c('red', 'green')) + 
  ggtitle('IOM Seat Price: 2001-2013')
```

If we clearly observe the linear approach fairly performs a good job of interpolation/ filling in missing values  for IOM seat prices .


Lets perform interpolation using a new approach i.e constant approach 

## Creating the uniform time series using the constant interpolation 
```{r}
uniform_cmeS_constant <- create_uniform_ts(df_cmeS, interpolation = c('constant'))
uniform_immS_constant <- create_uniform_ts(df_immS, interpolation = c('constant'))
uniform_iomS_constant <- create_uniform_ts(df_iomS, interpolation = c('constant'))

```


# lets see if there are any missing values after the constant interpolation for the CME seat prices data
```{r}
sum(is.na(uniform_cmeS_constant$constant))
```

# lets see if there are any missing values after the constant interpolation for the IMM seat prices data
```{r}
sum(is.na(uniform_immS_constant$constant))
```

# lets see if there are any missing values after the constant interpolation for the IOM seat prices data
```{r}
sum(is.na(uniform_iomS_constant$constant))
```

### Plotting the uniform time series where the missing values were filled in using the 'constant' interpolation 

# plotting the data for the CME seat prices after constant interpolation
```{r}
ggplot(uniform_cmeS_constant, aes(x=yearmon, y=constant, color=interpolated)) + 
  geom_point()  + scale_color_manual(values=c('red', 'green')) + 
  ggtitle('CME Seat Price: 2001-2013')

```

 The constant approach does not perform a good job of interpolation/ filling in missing values  for CME seat prices. The results from this approach have been worse compared to the linear interpolation 

# plotting the data for the IMM seat prices after constant interpolation
```{r}
ggplot(uniform_immS_constant, aes(x=yearmon, y=constant, color=interpolated)) + 
  geom_point() + scale_color_manual(values=c('red', 'green')) +
  ggtitle('IMM Seat Price: 2001-2013')

```


The constant approach does not perform a good job of interpolation/ filling in missing values  for IMM seat prices. The results from this approach have been worse compared to the linear interpolation 


# plotting the data for the IOM seat prices after constant interpolation
```{r}
ggplot(uniform_iomS_constant, aes(x=yearmon, y=constant, color=interpolated)) + 
  geom_point() + scale_color_manual(values=c('red', 'green')) + 
  ggtitle('IOM Seat Price: 2001-2013')
```

The constant approach does not perform a good job of interpolation/ filling in missing values  for IOM seat prices. The results from this approach have been worse compared to the linear interpolation 



Lets perform interpolation using a new approach i.e spline approach 

## Creating the uniform time series using the spline interpolation 
```{r}
uniform_cmeS_spline <- create_uniform_ts(df_cmeS, interpolation = c('spline'))
uniform_immS_spline <- create_uniform_ts(df_immS, interpolation = c('spline'))
uniform_iomS_spline <- create_uniform_ts(df_iomS, interpolation = c('spline'))

```


# lets see if there are any missing values after the spline interpolation for the CME seat prices data
```{r}
sum(is.na(uniform_cmeS_spline$spline))
```

# lets see if there are any missing values after the spline interpolation for the IMM seat prices data
```{r}
sum(is.na(uniform_immS_spline$spline))
```

# lets see if there are any missing values after the spline interpolation for the IOM seat prices data
```{r}
sum(is.na(uniform_iomS_spline$spline))
```

### Plotting the uniform time series where the missing values were filled in using the 'spline' interpolation 

# plotting the data for the CME seat prices after spline interpolation
```{r}
ggplot(uniform_cmeS_spline, aes(x=yearmon, y=spline, color=interpolated)) + 
  geom_point()  + scale_color_manual(values=c('red', 'green')) + 
  ggtitle('CME Seat Price: 2001-2013')

```

The spline approach does a fantastic job of interpolation/ filling in missing values  for CME seat prices. The results from this approach have been best so far compared to the constant or the linear interpolation approach

# plotting the data for the IMM seat prices after spline interpolation
```{r}
ggplot(uniform_immS_spline, aes(x=yearmon, y=spline, color=interpolated)) + 
  geom_point() + scale_color_manual(values=c('red', 'green')) +
  ggtitle('IMM Seat Price: 2001-2013')

```


The spline approach does a fantastic job of interpolation/ filling in missing values  for IMM seat prices. The results from this approach have been best so far compared to the constant or the linear interpolation approach


# plotting the data for the IOM seat prices after spline interpolation
```{r}
ggplot(uniform_iomS_spline, aes(x=yearmon, y=spline, color=interpolated)) + 
  geom_point() + scale_color_manual(values=c('red', 'green')) + 
  ggtitle('IOM Seat Price: 2001-2013')
```

The spline approach does a fantastic job of interpolation/ filling in missing values  for IMM seat prices. The results from this approach have been best so far compared to the constant or the linear interpolation approach

This particular part of the assignment was about smoothing the CME  time series data by filling in the missing values. I think spline performed an excellent job in reflecting the average/mean behavior of CME seat price for that month. 


The spline interpolation approach did the best job overall looking and comparing the plots with the constant and linear interpolation approaches .
Creating and plotting the time series after the linear or constant interpolation indicate obvious breaks in the time series and the spline interpolated data seems to be real time series from the plots.


spline interpolation seems to provide a much higher order of flexibility compared to constant or linear interpolation. This results in a time series imputation which is more reasonable. 

The main reason why spline performs better is because of the non-linearity as it tries to estimate the missing values by fitting the higher order polynomials to those values .











